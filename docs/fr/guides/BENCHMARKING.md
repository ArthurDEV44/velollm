# Guide de benchmarking VeloLLM

## AperÃ§u

VeloLLM inclut une suite de benchmarking complÃ¨te pour mesurer les performances d'infÃ©rence LLM sur diffÃ©rents matÃ©riels et configurations.

## DÃ©marrage rapide

### PrÃ©requis

1. **Ollama installÃ© et en cours d'exÃ©cution** :
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ```

2. **ModÃ¨le tÃ©lÃ©chargÃ©** :
   ```bash
   ollama pull llama3.2:3b
   ```

### ExÃ©cuter les benchmarks

```bash
# ExÃ©cuter avec le modÃ¨le par dÃ©faut (llama3.2:3b)
velollm benchmark

# SpÃ©cifier un modÃ¨le diffÃ©rent
velollm benchmark -m llama3.1:8b

# Sauvegarder les rÃ©sultats en JSON
velollm benchmark -o results.json

# Utiliser un backend diffÃ©rent (futur)
velollm benchmark -b llamacpp
```

## Benchmarks standard

La suite inclut trois benchmarks standard :

### 1. ComplÃ©tion courte (50 jetons)
**Prompt** : "Write a hello world program in Python"
**ItÃ©rations** : 5
**Objectif** : Mesurer la vitesse de gÃ©nÃ©ration de base

### 2. ComplÃ©tion moyenne (150 jetons)
**Prompt** : "Explain how neural networks learn through backpropagation in detail"
**ItÃ©rations** : 3
**Objectif** : Mesurer le dÃ©bit soutenu

### 3. GÃ©nÃ©ration de code (200 jetons)
**Prompt** : "Write a Rust function to compute the Fibonacci sequence using dynamic programming"
**ItÃ©rations** : 3
**Objectif** : Mesurer les performances de gÃ©nÃ©ration de code

## MÃ©triques collectÃ©es

### Jetons par seconde (tok/s)
- **DÃ©finition** : Nombre de jetons gÃ©nÃ©rÃ©s par seconde
- **Plus haut est mieux**
- **Plages typiques** :
  - CPU uniquement : 5-20 tok/s
  - GPU milieu de gamme (RTX 3060) : 30-60 tok/s
  - GPU haut de gamme (RTX 4090) : 80-150 tok/s

### Temps jusqu'au premier jeton (TTFT)
- **DÃ©finition** : Temps entre la requÃªte et le premier jeton gÃ©nÃ©rÃ©
- **Plus bas est mieux**
- **Composants** :
  - Temps d'Ã©valuation du prompt
  - Temps de gÃ©nÃ©ration du premier jeton
- **Plages typiques** :
  - Petits modÃ¨les (3B) : 50-200ms
  - Grands modÃ¨les (70B) : 500-2000ms

### Temps total
- **DÃ©finition** : DurÃ©e complÃ¨te de la requÃªte
- **Inclut** :
  - Traitement du prompt
  - Toute la gÃ©nÃ©ration de jetons
  - Formatage de la rÃ©ponse

## Exemple de sortie

```
ğŸš€ VeloLLM Benchmark Suite

Backend: ollama
Model: llama3.2:3b

Checking Ollama availability... âœ“

Running 3 benchmarks...

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Running benchmark: short_completion (5 iterations)
  Iteration 1/5... 82.3 tok/s (612ms)
  Iteration 2/5... 85.1 tok/s (593ms)
  Iteration 3/5... 84.7 tok/s (596ms)
  Iteration 4/5... 83.9 tok/s (602ms)
  Iteration 5/5... 84.2 tok/s (599ms)
  Average: 84.0 tok/s, TTFT: 127.3ms

Running benchmark: medium_completion (3 iterations)
  Iteration 1/3... 78.5 tok/s (1913ms)
  Iteration 2/3... 79.2 tok/s (1896ms)
  Iteration 3/3... 78.9 tok/s (1903ms)
  Average: 78.9 tok/s, TTFT: 145.6ms

Running benchmark: code_generation (3 iterations)
  Iteration 1/3... 76.3 tok/s (2621ms)
  Iteration 2/3... 77.1 tok/s (2596ms)
  Iteration 3/3... 76.8 tok/s (2605ms)
  Average: 76.7 tok/s, TTFT: 152.1ms

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Benchmark Summary

short_completion:
  Tokens/s: 84.0
  TTFT: 127.3ms
  Total tokens: 252
  Total time: 3.0s

medium_completion:
  Tokens/s: 78.9
  TTFT: 145.6ms
  Total tokens: 447
  Total time: 5.7s

code_generation:
  Tokens/s: 76.7
  TTFT: 152.1ms
  Total tokens: 593
  Total time: 7.7s

Overall Average:
  Tokens/s: 79.9
  TTFT: 141.7ms

ğŸ’¡ Tip: Use -o <file> to save results to JSON
```

## Format de sortie JSON

```json
[
  {
    "config": {
      "name": "short_completion",
      "model": "llama3.2:3b",
      "prompt": "Write a hello world program in Python",
      "max_tokens": 50,
      "iterations": 5
    },
    "tokens_per_second": 84.0,
    "time_to_first_token_ms": 127.3,
    "total_time_ms": 3002.5,
    "total_tokens": 252,
    "prompt_eval_count": 12,
    "eval_count": 50,
    "timestamp": "2025-01-15T10:30:00Z"
  },
  ...
]
```

## Comparer les rÃ©sultats

### Avant/AprÃ¨s optimisation

```bash
# ExÃ©cuter la ligne de base
velollm benchmark -o baseline.json

# Appliquer les optimisations
velollm optimize -o velollm.sh
source velollm.sh

# ExÃ©cuter optimisÃ©
velollm benchmark -o optimized.json

# Comparer (manuel)
jq '.[0].tokens_per_second' baseline.json
jq '.[0].tokens_per_second' optimized.json
```

### Entre diffÃ©rents matÃ©riels

CrÃ©er une base de donnÃ©es de benchmarks :

```bash
# Sur chaque systÃ¨me
velollm detect > hardware.json
velollm benchmark -o benchmark.json

# Organiser
mkdir benchmarks/rtx-4090
mv hardware.json benchmarks/rtx-4090/
mv benchmark.json benchmarks/rtx-4090/
```

## Utilisation avancÃ©e

### Benchmarks personnalisÃ©s

CrÃ©er votre propre configuration de benchmark :

```rust
use velollm_benchmarks::{BenchmarkConfig, BenchmarkRunner};

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let config = BenchmarkConfig {
        name: "custom_test".to_string(),
        model: "llama3.2:3b".to_string(),
        prompt: "Votre prompt personnalisÃ© ici".to_string(),
        max_tokens: 100,
        iterations: 5,
    };

    let runner = BenchmarkRunner::new("ollama");
    let result = runner.run(&config).await?;

    println!("Tokens/s: {}", result.tokens_per_second);
    Ok(())
}
```

### DiffÃ©rents modÃ¨les

Tester plusieurs modÃ¨les :

```bash
for model in llama3.2:1b llama3.2:3b llama3.1:8b; do
    echo "=== Testing $model ==="
    velollm benchmark -m $model -o results-$model.json
done
```

## InterprÃ©ter les rÃ©sultats

### Indicateurs de bonnes performances

âœ… **Jetons/s Ã©levÃ©s** : Utilisation efficace du GPU
âœ… **TTFT faible** : Traitement rapide du prompt
âœ… **CohÃ©rence entre itÃ©rations** : Performance stable

### ProblÃ¨mes de performance

âŒ **Jetons/s faibles** : VÃ©rifier l'utilisation GPU, pression mÃ©moire
âŒ **TTFT Ã©levÃ©** : Prompt trop long ou encodeur de prompt lent
âŒ **Variance entre itÃ©rations** : Limitation thermique ou processus en arriÃ¨re-plan

## DÃ©pannage

### "Ollama is not running"

```bash
# VÃ©rifier le statut d'Ollama
systemctl status ollama  # Linux
ollama serve            # DÃ©marrage manuel

# VÃ©rifier que le modÃ¨le est disponible
ollama list
ollama pull llama3.2:3b
```

### "Model not found"

```bash
# Lister les modÃ¨les disponibles
ollama list

# TÃ©lÃ©charger le modÃ¨le
ollama pull llama3.2:3b
```

### Performance lente

**VÃ©rifier l'utilisation GPU** :
```bash
# NVIDIA
nvidia-smi

# AMD
rocm-smi
```

**VÃ©rifier la VRAM** :
- S'assurer que le modÃ¨le tient dans la VRAM
- Fermer les autres applications GPU
- Essayer un modÃ¨le plus petit ou la quantification

**VÃ©rifier l'utilisation CPU** :
```bash
top
htop
```

### Erreurs rÃ©seau

```bash
# VÃ©rifier l'API Ollama
curl http://localhost:11434/api/tags

# Essayer un port diffÃ©rent
velollm benchmark --ollama-url http://localhost:11434
```

## Conseils d'optimisation des performances

### 1. Utiliser une taille de modÃ¨le appropriÃ©e

- **<8Go VRAM** : llama3.2:1b ou 3b
- **8-16Go VRAM** : llama3.1:8b (quantification Q4)
- **>16Go VRAM** : llama3.1:13b ou plus grand

### 2. Optimiser les paramÃ¨tres Ollama

```bash
# Augmenter la fenÃªtre de contexte
export OLLAMA_NUM_CTX=4096

# Taille de lot
export OLLAMA_NUM_BATCH=512

# Keep alive
export OLLAMA_KEEP_ALIVE=5m
```

### 3. DÃ©chargement GPU

```bash
# DÃ©charger toutes les couches vers le GPU
export OLLAMA_NUM_GPU=99
```

### 4. RÃ©duire la charge en arriÃ¨re-plan

- Fermer les navigateurs
- ArrÃªter les autres applications GPU
- DÃ©sactiver les effets de bureau accÃ©lÃ©rÃ©s par GPU

## Prochaines Ã©tapes

- Comparer vos rÃ©sultats avec les benchmarks communautaires
- ExpÃ©rimenter avec diffÃ©rents modÃ¨les
- Essayer le dÃ©codage spÃ©culatif (Ã  venir en Phase 2)
- Contribuer vos rÃ©sultats Ã  la base de donnÃ©es de benchmarks VeloLLM

## RÃ©fÃ©rences

- [Documentation API Ollama](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [ROADMAP VeloLLM](../../ROADMAP.md)
- [Guide de dÃ©tection matÃ©rielle](hardware_detection.md)
