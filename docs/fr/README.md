# VeloLLM

**Pilote automatique pour l'infÃ©rence LLM locale** - Optimisation des performances sans configuration pour Ollama, llama.cpp et plus encore.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Rust](https://img.shields.io/badge/rust-%23000000.svg?style=flat&logo=rust&logoColor=white)](https://www.rust-lang.org/)
[![CI](https://github.com/ArthurDEV44/velollm/actions/workflows/ci.yml/badge.svg)](https://github.com/ArthurDEV44/velollm/actions/workflows/ci.yml)

## Le problÃ¨me

L'infÃ©rence LLM locale est **35 Ã  50 fois plus lente** que les solutions cloud (vLLM, Morph) malgrÃ© un matÃ©riel comparable. VeloLLM comble cet Ã©cart en apportant des optimisations de niveau production aux dÃ©ploiements locaux.

**Ã‰tat actuel** :
- Cloud (vLLM) : 10 000+ jetons/s avec dÃ©codage spÃ©culatif
- Local (Ollama) : 200-300 jetons/s (utilisateur moyen)

**Objectif de VeloLLM** : RÃ©duire cet Ã©cart de performance grÃ¢ce Ã  des optimisations intelligentes et automatiques.

---

## DÃ©marrage rapide

### Installation

```bash
# Depuis crates.io (bientÃ´t disponible)
cargo install velollm

# Depuis les sources
git clone https://github.com/ArthurDEV44/velollm.git
cd velollm
cargo install --path velollm-cli
```

### Utilisation

```bash
# 1. DÃ©tecter votre matÃ©riel
velollm detect

# 2. Optimiser la configuration Ollama
velollm optimize --dry-run  # AperÃ§u des modifications
velollm optimize -o velollm.sh
source velollm.sh

# 3. Benchmarker les performances
velollm benchmark

# 4. Comparer avant/aprÃ¨s
velollm benchmark --compare baseline.json optimized.json
```

---

## FonctionnalitÃ©s

### Phase 1 (MVP - Actuelle)

- **DÃ©tection matÃ©rielle** : DÃ©tection automatique du GPU (NVIDIA/AMD/Apple), CPU, RAM
- **Auto-configuration Ollama** : Optimiser l'utilisation de la VRAM, la taille des lots, la fenÃªtre de contexte
- **Suite de benchmarks** : Mesurer les jetons/s, le temps jusqu'au premier jeton, l'utilisation de la mÃ©moire
- **DÃ©codage spÃ©culatif** : AccÃ©lÃ©ration de 1,5 Ã  2,5x via l'intÃ©gration d'un modÃ¨le brouillon

### Phase 2 (Mois 4-6)

- **PagedAttention** : RÃ©duction de 70% de la fragmentation du cache KV
- **Batching continu** : GÃ©rer efficacement 4 Ã  8 utilisateurs simultanÃ©s
- **Hybride CPU-GPU** : Placement intelligent des couches et dÃ©chargement
- **Multi-backend** : Support pour llama.cpp, LocalAI, vLLM

### Phase 3 (Mois 7-12)

- **Interface graphique** : Surveillance des performances en temps rÃ©el
- **IntÃ©grations IDE** : VSCode, Continue.dev, Cursor
- **Support Mamba/MoE** : Architectures de modÃ¨les de nouvelle gÃ©nÃ©ration
- **Place de marchÃ© de configurations** : Base de donnÃ©es d'optimisation pilotÃ©e par la communautÃ©

Voir [ROADMAP.md](../../ROADMAP.md) pour tous les dÃ©tails.

---

## RÃ©sultats des benchmarks

### Performance attendue (Objectifs Phase 1)

| MatÃ©riel | ModÃ¨le | Base | VeloLLM | AccÃ©lÃ©ration |
|----------|--------|------|---------|--------------|
| RTX 4090 24GB | Llama 3.1 8B | ~28 tok/s | 60-70 tok/s | 2,1-2,5x |
| RTX 3060 12GB | Llama 3.2 3B | ~35 tok/s | 70-85 tok/s | 2,0-2,4x |
| M2 Max 32GB | Llama 3.1 8B | ~22 tok/s | 45-55 tok/s | 2,0-2,5x |

Voir [BENCHMARKS.md](../../BENCHMARKS.md) pour la mÃ©thodologie et les rÃ©sultats dÃ©taillÃ©s.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Couche d'orchestration VeloLLM             â”‚
â”‚  â€¢ DÃ©tection matÃ©rielle                         â”‚
â”‚  â€¢ Auto-configuration                           â”‚
â”‚  â€¢ Profilage des performances                   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚          â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama  â”‚ â”‚llama.cppâ”‚ â”‚LocalAI â”‚ â”‚  vLLM  â”‚
â”‚ Adapteur â”‚ â”‚Adapteur â”‚ â”‚Adapteurâ”‚ â”‚Adapteurâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Technologies principales** :
- **Backend** : Rust (optimisations critiques pour les performances)
- **CLI/Outils** : TypeScript/Node.js (expÃ©rience dÃ©veloppeur)
- **Bindings** : Python (compatibilitÃ© Ã©cosystÃ¨me ML)

---

## Statut du dÃ©veloppement

**Phase actuelle** : Phase 1 - DÃ©veloppement MVP

| TÃ¢che | Statut |
|-------|--------|
| Configuration du dÃ©pÃ´t | âœ… TerminÃ© |
| SystÃ¨me de build | âœ… TerminÃ© |
| DÃ©tection matÃ©rielle | â³ PlanifiÃ© |
| Suite de benchmarks | â³ PlanifiÃ© |
| PoC dÃ©codage spÃ©culatif | â³ PlanifiÃ© |
| Optimisation Ollama | â³ PlanifiÃ© |

Suivre la progression : [TODO.md](../../TODO.md)

---

## Contribution

Nous accueillons les contributions ! VeloLLM est en dÃ©veloppement prÃ©coce et a besoin d'aide pour :

- **Optimisations principales** : PagedAttention, dÃ©codage spÃ©culatif
- **Adaptateurs backend** : Support pour plus de moteurs d'infÃ©rence
- **Benchmarking** : Tests sur diverses configurations matÃ©rielles
- **Documentation** : Guides, tutoriels, documentation API

Voir [CONTRIBUTING.md](../../CONTRIBUTING.md) pour les directives.

---

## Feuille de route

**Phase 1 (Mois 1-3)** : MVP avec accÃ©lÃ©ration 2-3x
- IntÃ©gration du dÃ©codage spÃ©culatif
- Auto-configuration Ollama
- Benchmarking de base

**Phase 2 (Mois 4-6)** : Optimisations avancÃ©es (3-5x)
- ImplÃ©mentation PagedAttention
- Batching continu pour le local
- Support multi-backend

**Phase 3 (Mois 7-12)** : Ã‰cosystÃ¨me (5-10x)
- Interface graphique et surveillance
- IntÃ©grations IDE
- Alternatives d'architecture (Mamba, MoE)

DÃ©tails complets : [ROADMAP.md](../../ROADMAP.md)

---

## Pourquoi VeloLLM ?

### DiffÃ©renciation

| FonctionnalitÃ© | Ollama | vLLM | LM Studio | VeloLLM |
|----------------|--------|------|-----------|---------|
| Cible | SimplicitÃ© | Prod cloud | Utilisateurs desktop | Performance locale |
| DÃ©codage spÃ©culatif | âŒ | âŒ | âœ… | âœ… Auto-configurÃ© |
| PagedAttention | âŒ | âœ… | âŒ | âœ… AdaptÃ© local |
| Batching continu | âŒ | âœ… | âŒ | âœ… Multi-utilisateur |
| Auto-optimisation | âŒ | âŒ | Partiel | âœ… AdaptÃ© au matÃ©riel |
| Open Source | âœ… | âœ… | âŒ | âœ… |

### Proposition de valeur

**VeloLLM = "Pilote automatique pour l'infÃ©rence IA locale"**

1. **Sans configuration** : DÃ©tecte le matÃ©riel, applique automatiquement les paramÃ¨tres optimaux
2. **AdaptÃ© au matÃ©riel** : S'adapte dynamiquement (ordinateur portable vs station de travail vs serveur)
3. **Multi-backend** : Fonctionne avec Ollama, llama.cpp, LocalAI de maniÃ¨re transparente
4. **Transparent** : Surveillance dÃ©taillÃ©e, mÃ©triques, explications des optimisations
5. **PilotÃ© par la communautÃ©** : Open source, extensible, bien documentÃ©

---

## Recherche & RÃ©fÃ©rences

Ce projet s'appuie sur :

- [llama.cpp](https://github.com/ggml-org/llama.cpp) : Fondation pour le dÃ©codage spÃ©culatif
- [vLLM](https://github.com/vllm-project/vllm) : PagedAttention et batching continu
- [Ollama](https://github.com/ollama/ollama) : ExpÃ©rience utilisateur et conception API
- [Mamba](https://github.com/state-spaces/mamba) : Exploration d'architectures alternatives

Articles clÃ©s :
- [PagedAttention](https://blog.vllm.ai/2025/09/05/anatomy-of-vllm.html) : Optimisation de la mÃ©moire
- [DÃ©codage spÃ©culatif](https://arxiv.org/abs/2211.17192) : AccÃ©lÃ©ration de l'infÃ©rence

---

## Licence

Licence MIT - voir [LICENSE](../../LICENSE) pour les dÃ©tails.

---

## Contact

- **Issues** : [GitHub Issues](https://github.com/ArthurDEV44/velollm/issues)
- **Discussions** : [GitHub Discussions](https://github.com/ArthurDEV44/velollm/discussions)
- **DÃ©pÃ´t** : [github.com/ArthurDEV44/velollm](https://github.com/ArthurDEV44/velollm)

---

**Statut** : ğŸš§ DÃ©veloppement prÃ©coce - MVP Phase 1 en cours

Construit avec â¤ï¸ par la communautÃ© VeloLLM.
